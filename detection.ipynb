{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset original source:\n",
    "\n",
    "- [Kaggle](https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection)\n",
    "\n",
    "Build a predictive model\n",
    "\n",
    "- Compare: NB, KNN, SVM\n",
    "\n",
    "Theorical sources\n",
    "\n",
    "- [NB](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "- [KNN](https://www.codecademy.com/learn/introduction-to-supervised-learning-skill-path/modules/k-nearest-neighbors-skill-path/cheatsheet)\n",
    "- [SVM](https://es.wikipedia.org/wiki/M%C3%A1quinas_de_vectores_de_soporte)\n",
    "\n",
    "Sklearn algorithm references\n",
    "\n",
    "- [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html)\n",
    "- [One Hot Encoder](https://datagy.io/sklearn-one-hot-encode/)\n",
    "- [Text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "- [NB](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "- [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Sarcasm_Headlines_Dataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_name, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(text):\n",
    "    tokenizer = lambda text : nltk.word_tokenize(text)\n",
    "    to_lower = lambda text : text.lower()\n",
    "    parse_url = lambda text : re.sub('http\\S+' , '' , text)\n",
    "    strip = lambda text : text.strip()\n",
    "    to_raw = lambda text : re.sub('[^a-z\\s]', '', text)  #drop any symbol except a-z\n",
    "\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    ws = tokenizer(text)\n",
    "    ws = [to_lower(w) for w in ws]\n",
    "    ws = [parse_url(w) for w in ws]\n",
    "    ws = [strip(w) for w in ws]\n",
    "    ws = [to_raw(w) for w in ws]\n",
    "    ws = [w for w in ws if w not in en_stop]\n",
    "    ws = [w for w in ws if wordnet.synsets(w)] # known synomous of this word\n",
    "    \n",
    "    return ' '.join(ws).strip()\n",
    "    \n",
    "df['cleaned_headline'] = df['headline'].apply(denoise)\n",
    "\n",
    "df[['headline', 'cleaned_headline']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize headline tokens with WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sarcastic_headline_df = df[df['is_sarcastic'] == 0]['cleaned_headline']\n",
    "sarcastic_headline_df = df[df['is_sarcastic'] == 1]['cleaned_headline']\n",
    "\n",
    "non_sarcastic_headline_np = non_sarcastic_headline_df.values\n",
    "sarcastic_headline_np = sarcastic_headline_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(non_sarcastic_headline_np)\n",
    "plt.figure(figsize = (10,10))\n",
    "wc = WordCloud(width = 2000 , height = 1000 , max_words = 500).generate(text)\n",
    "plt.axis('off')\n",
    "plt.title('Worcloud of non sarcastic words')\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(sarcastic_headline_np)\n",
    "plt.figure(figsize = (10,10))\n",
    "wc = WordCloud(width = 2000 , height = 1000 , max_words = 500).generate(text)\n",
    "plt.axis('off')\n",
    "plt.title('Worcloud of sarcastic words')\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The headline transmitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_transmitter = lambda source : source.split('.')[1]\n",
    "df['transmitter'] = df['article_link'].apply(extract_transmitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('article_link', inplace=True, errors='ignore', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: encode transmitter columns\n",
    "df['transmitter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "transmitter_transformed = ohe.fit_transform(df[['transmitter']])\n",
    "print(ohe.categories_)\n",
    "print(transmitter_transformed.toarray()[0:5])\n",
    "# this could be a way to transform this columns but\n",
    "# I'll use ColumnTransfomer class to acomplish this\n",
    "# df[ohe.categories_[0]] = transmitter_transformed.toarray()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['transmitter']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "transformed = transformer.fit_transform(df)\n",
    "\n",
    "transformed_df = pd.DataFrame(\n",
    "    transformed, \n",
    "    columns=transformer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "transformed_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "432797e8a5a6cd19b4d9d041766ad6391ea06ff095d24242b682a4d02bf70dba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
